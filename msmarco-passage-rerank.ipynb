{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import json\n",
    "import wget\n",
    "import requests\n",
    "import tarfile\n",
    "\n",
    "import argparse\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import transformers\n",
    "from peft import LoraConfig\n",
    "from transformers import (AutoTokenizer, \n",
    "                          AutoModelForCausalLM,\n",
    "                          BitsAndBytesConfig)\n",
    "import utils\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(description='Rerank')\n",
    "\n",
    "parser.add_argument('--model_name', type=str, default='facebook/opt-125m')\n",
    "parser.add_argument('--collection', type=str, default='msmarco-passage')\n",
    "parser.add_argument('--collection_dir', type=str, default='./collections/msmarco-passage')\n",
    "parser.add_argument('--seed',type=int, default=2023)\n",
    "parser.add_argument('--batch_size', type=int, default=256)\n",
    "parser.add_argument('--max_len', type=int, default=40)\n",
    "parser.add_argument('--lr', type=float, default=1e-5)\n",
    "parser.add_argument('--max_epochs', type=int, default=10)\n",
    "parser.add_argument('--use_cuda', type=bool, default=True)\n",
    "parser.add_argument('--k', type=int, default=100, help='top k')\n",
    "parser.add_argument('--k1', type=float, default=1.5, help='BM25 parameter')\n",
    "parser.add_argument('--b', type=float, default=0.75, help='BM25 parameter')\n",
    "\n",
    "parser.add_argument\n",
    "\n",
    "config = parser.parse_args([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection_path = os.path.join(config.collection_dir, 'collection.tsv')\n",
    "queries_tr_path = os.path.join(config.collection_dir, 'queries.train.tsv')\n",
    "qrels_tr_path = os.path.join(config.collection_dir, 'qrels.train.tsv')\n",
    "qrels_dev_path = os.path.join(config.collection_dir, 'qrels.dev.tsv')\n",
    "queries_dev_path = os.path.join(config.collection_dir, 'queries.dev.tsv')\n",
    "queries_eval_path = os.path.join(config.collection_dir, 'queries.eval.tsv')\n",
    "top1000_tr_path = os.path.join(config.collection_dir, 'top1000.train.txt')\n",
    "top1000_dev_path = os.path.join(config.collection_dir, 'top1000.dev')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to /home/work/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "\n",
    "set_seed(config.seed)\n",
    "\n",
    "hf_token = open('./hf_token.txt', 'r', encoding='utf-8').read()\n",
    "os.system(f'huggingface-cli login --token {hf_token}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset, TensorDataset, IterableDataset\n",
    "\n",
    "# Encode Dataset For Reranking\n",
    "class MarcoEncodeDataset(Dataset):\n",
    "    def __init__(self, collection_dir, tokenizer, mode='train', q_max_len=128, p_max_len=128):\n",
    "        self.collection_dir = collection_dir\n",
    "        self.tokenizer = tokenizer\n",
    "        self.mode = mode\n",
    "        self.q_max_len = q_max_len\n",
    "        self.p_max_len = p_max_len\n",
    "        # load data\n",
    "        passages_path = os.path.join(collection_dir, 'collection.tsv')\n",
    "        queries_path = os.path.join(collection_dir, f'queries.{mode}.tsv')\n",
    "        qrels_path = os.path.join(collection_dir, f'qrels.{mode}.tsv')\n",
    "        \n",
    "        self.passages = pd.read_csv(passages_path, sep='\\t', header=None, names=['pid', 'passage'], index_col='pid')\n",
    "        self.queries = pd.read_csv(queries_path, sep='\\t', header=None, names=['qid', 'query'], index_col='qid')\n",
    "        self.relations = pd.read_csv(qrels_path, sep='\\t', header=None, names=['qid', '0', 'pid', 'label'])\n",
    "        if self.mode == 'train':\n",
    "            top1000_path = os.path.join(collection_dir, f'top1000.{mode}.txt')\n",
    "            top1000_dict = utils.read_top1000(top1000_path)\n",
    "            self.top1000 = pd.DataFrame(list(top1000_dict.items()), columns=['qid', 'pid'])\n",
    "        else:\n",
    "            top1000_path = os.path.join(collection_dir, f'top1000.{mode}')\n",
    "            self.top1000 = pd.read_csv(top1000_path, sep='\\t', header=None, names=['qid', 'pid', 'query', 'passage'])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.top1000)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        x = self.top1000.iloc[idx]\n",
    "        query = self.queries.loc[x.qid].query\n",
    "        passage = self.passages.loc[x.pid].passage \n",
    "        label = 0 if self.relations.loc[(self.relations['qid'] == x.qid) & (self.relations['pid'] == x.pid)].empty else 1\n",
    "        \n",
    "        encode_query = self.tokenizer.encode_plus(\n",
    "            query,\n",
    "            max_length=self.q_max_len,\n",
    "            truncation='only_first',\n",
    "            # return_token_type_ids=True,\n",
    "            return_attention_mask=True,\n",
    "            padding='max_length',\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        encoded_psg = self.tokenizer.encode_plus(\n",
    "            passage,\n",
    "            max_length=self.p_max_len,\n",
    "            truncation='only_first',\n",
    "            # return_token_type_ids=True,\n",
    "            return_attention_mask=True,\n",
    "            padding='max_length',\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        encoded = {\n",
    "            'qid': x.qid,\n",
    "            'pid': x.pid,\n",
    "            'q_input_ids': encode_query['input_ids'], # query\n",
    "            'p_input_ids': encoded_psg['input_ids'], # passage\n",
    "            'q_attn_msk': encode_query['attention_mask'],\n",
    "            'p_attn_msk': encoded_psg['attention_mask'], \n",
    "            'label': torch.LongTensor([label]),\n",
    "        }\n",
    "        \n",
    "        return encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "LLAMA_MODEL_LIST = [\n",
    "    'meta-llama/Llama-2-7b',\n",
    "    'meta-llama/Llama-2-7b-hf',\n",
    "    'meta-llama/Llama-2-7b-chat',\n",
    "    'meta-llama/Llama-2-7b-chat-hf',\n",
    "    'meta-llama/Llama-2-13b',\n",
    "    'meta-llama/Llama-2-13b-hf'\n",
    "    'meta-llama/Llama-2-13b-chat',\n",
    "    'meta-llama/Llama-2-13b-chat-hf',\n",
    "    'meta-llama/Llama-2-70b',\n",
    "    'meta-llama/Llama-2-70b-hf',\n",
    "    'meta-llama/Llama-2-70b-chat',\n",
    "    'meta-llama/Llama-2-70b-chat-hf',\n",
    "]\n",
    "\n",
    "# huggingface-cli login --token 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\n",
    "\n",
    "class LlaMAReranker:\n",
    "    def __init__(self, model_name, use_cuda, batch_size, n_gpu, device, mode='train'):\n",
    "        self.use_cuda = use_cuda\n",
    "        self.model_name = model_name\n",
    "        self.mode = mode\n",
    "        self.batch_size = batch_size\n",
    "        self.n_gpu = n_gpu\n",
    "        self.device = device\n",
    "        # self.device = torch.device('cuda' if torch.cuda.is_available() & use_cuda else 'cpu')\n",
    "\n",
    "        self.model = self.load_model(self.model_name, self.use_cuda)\n",
    "        self.tokenizer = self.load_tokenizer(config.model_name)\n",
    "        self.encode_dataset = MarcoEncodeDataset(config.collection_dir, self.tokenizer, mode=self.mode)\n",
    "        self.encode_dataloader = DataLoader(self.encode_dataset, batch_size=self.batch_size, num_workers=4*self.n_gpu)\n",
    "    \n",
    "    def load_model(self, model_name:str, use_cuda:bool):\n",
    "        model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float32).to(self.device)\n",
    "        # model.config.pad_token_id = model.config.bos_token_id\n",
    "        model.config.use_cache=True\n",
    "        model.resize_token_embeddings(len(self.tokenizer))\n",
    "\n",
    "        return model\n",
    "    \n",
    "    def load_tokenizer(self, model_name:str):\n",
    "        tokenizer =  AutoTokenizer.from_pretrained(model_name)\n",
    "        tokenizer.pad_token = tokenizer.bos_token\n",
    "        return tokenizer\n",
    "    \n",
    "    def rerank(self, encoded):\n",
    "        q_input_ids = encoded['q_input_ids'].squeeze(dim=1)\n",
    "        p_input_ids = encoded['p_input_ids'].squeeze(dim=1)\n",
    "        q_attn_msk = encoded['q_attn_msk'].squeeze(dim=1)\n",
    "        p_attn_msk = encoded['p_attn_msk'].squeeze(dim=1)\n",
    "        input_ids = torch.cat([q_input_ids, p_input_ids], dim=1).to(self.device)\n",
    "        attn_msk = torch.cat([q_attn_msk, p_attn_msk], dim=1).to(self.device)\n",
    "        inputs = {'input_ids': input_ids, 'attention_mask': attn_msk}\n",
    "        # q_input_ids = q_input_ids.to(device)\n",
    "        # p_input_ids = p_input_ids.to(device)\n",
    "        # p_len = len(p_input_ids)\n",
    "        with torch.no_grad():\n",
    "            logits = self.model(**inputs).logits\n",
    "        \n",
    "        return logits\n",
    "            \n",
    "    # 그냥 일반 rerank (이때 assert q_max_len == p_max_len)\n",
    "    def score(self, encoded):\n",
    "        input_ids = torch.cat([encoded['q_input_ids'], encoded['p_input_ids']], dim=1).to(self.device)\n",
    "        attn_msk = torch.cat([encoded['q_attn_msk'], encoded['p_attn_msk']], dim=1).to(self.device)\n",
    "        p_len = len(encoded['p_input_ids'])\n",
    "        inputs = {'input_ids': input_ids, 'attention_mask': attn_msk}\n",
    "        with torch.no_grad():\n",
    "            logits = self.model(**inputs).logits\n",
    "        \n",
    "        score = logits.sum().item()\n",
    "        return score\n",
    "\n",
    "# torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_GPU = os.cpu_count()\n",
    "DEVICE = torch.device('cuda:0' if torch.cuda.is_available() & config.use_cuda else 'cpu')\n",
    "llama_reranker = LlaMAReranker(model_name=LLAMA_MODEL_LIST[1], use_cuda=config.use_cuda, batch_size=1, n_gpu=4*N_GPU, device=DEVICE, mode='dev')\n",
    "model = llama_reranker.model\n",
    "tokenizer = llama_reranker.tokenizer\n",
    "# dev_dataset = llama_reranker.encode_dataset \n",
    "# dev_dataloader = llama_reranker.encode_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# marco_encoded_train = MarcoEncodeDataset(config.collection_dir, tokenizer)\n",
    "marco_encoded_dev = MarcoEncodeDataset(config.collection_dir, tokenizer, mode='dev')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/work/YAI-Summer/miniconda3/envs/rerank_clone/lib/python3.8/site-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 32 worker processes in total. Our suggested max number of worker in current system is 8, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    }
   ],
   "source": [
    "# train_dataloader = DataLoader(marco_encoded_train, batch_size=config.batch_size, num_workers=4*n_gpu)\n",
    "dev_dataloader = DataLoader(marco_encoded_dev, batch_size=config.batch_size, num_workers=4*N_GPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a = next(iter(train_dataloader))\n",
    "a = next(iter(dev_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([256, 128])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a['p_input_ids'].squeeze(dim=1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(50265, 4096)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "model(a['p_input_ids'].squeeze(dim=1).to(DEVICE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m input_ids \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mcat([a[\u001b[39m'\u001b[39;49m\u001b[39mq_input_ids\u001b[39;49m\u001b[39m'\u001b[39;49m]\u001b[39m.\u001b[39;49msqueeze(dim\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m), a[\u001b[39m'\u001b[39;49m\u001b[39mp_input_ids\u001b[39;49m\u001b[39m'\u001b[39;49m]\u001b[39m.\u001b[39;49msqueeze(dim\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)], dim\u001b[39m=\u001b[39;49m\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m)\u001b[39m.\u001b[39;49mto(DEVICE)\n\u001b[1;32m      2\u001b[0m attn_msk \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat([a[\u001b[39m'\u001b[39m\u001b[39mq_attn_msk\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39msqueeze(dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m), a[\u001b[39m'\u001b[39m\u001b[39mp_attn_msk\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39msqueeze(dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)], dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mto(DEVICE)\n\u001b[1;32m      3\u001b[0m inputs \u001b[39m=\u001b[39m {\u001b[39m'\u001b[39m\u001b[39minput_ids\u001b[39m\u001b[39m'\u001b[39m: input_ids, \u001b[39m'\u001b[39m\u001b[39mattention_mask\u001b[39m\u001b[39m'\u001b[39m: attn_msk}\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "input_ids = torch.cat([a['q_input_ids'].squeeze(dim=1), a['p_input_ids'].squeeze(dim=1)], dim=-1).to(DEVICE)\n",
    "attn_msk = torch.cat([a['q_attn_msk'].squeeze(dim=1), a['p_attn_msk'].squeeze(dim=1)], dim=1).to(DEVICE)\n",
    "inputs = {'input_ids': input_ids, 'attention_mask': attn_msk}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode_query = tokenizer.encode_plus(\n",
    "#             'what is query',\n",
    "#             max_length=128,\n",
    "#             truncation='only_first',\n",
    "#             # return_token_type_ids=True,\n",
    "#             return_attention_mask=True,\n",
    "#             padding='max_length',\n",
    "#             return_tensors='pt'\n",
    "#         )\n",
    "# encode_query\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, encodeds in enumerate(train_dataloader):\n",
    "    logits, y = llama_reranker.rerank(encodeds)\n",
    "    loss_fn = \n",
    "    loss = loss_fn(logits, y)\n",
    "    model.zero_grad()\n",
    "    loss.backward()\n",
    "\n",
    "    if"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://huggingface.co/models?search=gpt+neo\n",
    "GPT_PRETRAINED_MODEL_LIST = [\n",
    "    'EleutherAI/gpt-neo-125m',\n",
    "    'EleutherAI/gpt-neo-2.7B',\n",
    "    'EleutherAI/gpt-neo-1.3B'\n",
    "]\n",
    "\n",
    "class GPTReranker:\n",
    "    def __init__(self):\n",
    "        self.model = self.load_model(config.model_name, config.use_cuda)\n",
    "        self.tokenizer = self.load_tokenizer(config.model_name)\n",
    "        # tokenizer.pad_token = tokenizer.eos_token\n",
    "        self.model.eval()\n",
    "    \n",
    "    def load_model(self, model_name:str, use_cuda:bool):\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() & use_cuda else 'cpu')\n",
    "        model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float32).to(device)\n",
    "        model.config.use_cache=True\n",
    "        return model\n",
    "    \n",
    "    def load_tokenizer(self, model_name:str):\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        return tokenizer\n",
    "    \n",
    "    def _get_prompt(self, query)\n",
    "    \n",
    "    def rerank(self, query, texts):\n",
    "        prompt =  f\"Please generate a query based on the following passage: {texts}\"\n",
    "\n",
    "# model = AutoModelForCausalLM.from_pretrained('EleutherAI/gpt-neo-125m')\n",
    "# tokenizer = AutoTokenizer.from_pretrained('EleutherAI/gpt-neo-125m')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rerank_clone",
   "language": "python",
   "name": "rerank_clone"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "vscode": {
   "interpreter": {
    "hash": "3f4f61d22295b4c336726dde2eefd2e957bb6ed051d61f1c4bb0fff9efabf191"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
