{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import json\n",
    "import wget\n",
    "import requests\n",
    "import tarfile\n",
    "import argparse\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import transformers\n",
    "from peft import LoraConfig\n",
    "from transformers import (AutoTokenizer, \n",
    "                          AutoModelForCausalLM,\n",
    "                          BitsAndBytesConfig)\n",
    "import utils\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(description='Rerank')\n",
    "\n",
    "parser.add_argument('--model_name', type=str, default='facebook/opt-125m')\n",
    "parser.add_argument('--collection', type=str, default='msmarco-passage')\n",
    "parser.add_argument('--collection_dir', type=str, default='./collections/msmarco-passage')\n",
    "parser.add_argument('--seed',type=int, default=2023)\n",
    "parser.add_argument('--batch_size', type=int, default=256)\n",
    "parser.add_argument('--max_len', type=int, default=40)\n",
    "parser.add_argument('--lr', type=float, default=1e-5)\n",
    "parser.add_argument('--max_epochs', type=int, default=10)\n",
    "parser.add_argument('--use_cuda', type=bool, default=True)\n",
    "parser.add_argument('--k', type=int, default=100, help='top k')\n",
    "parser.add_argument('--k1', type=float, default=1.5, help='BM25 parameter')\n",
    "parser.add_argument('--b', type=float, default=0.75, help='BM25 parameter')\n",
    "\n",
    "parser.add_argument\n",
    "\n",
    "config = parser.parse_args([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection_path = os.path.join(config.collection_dir, 'collection.tsv')\n",
    "queries_tr_path = os.path.join(config.collection_dir, 'queries.train.tsv')\n",
    "qrels_tr_path = os.path.join(config.collection_dir, 'qrels.train.tsv')\n",
    "qrels_dev_path = os.path.join(config.collection_dir, 'qrels.dev.tsv')\n",
    "queries_dev_path = os.path.join(config.collection_dir, 'queries.dev.tsv')\n",
    "queries_eval_path = os.path.join(config.collection_dir, 'queries.eval.tsv')\n",
    "top1000_tr_path = os.path.join(config.collection_dir, 'top1000.train.txt')\n",
    "top1000_dev_path = os.path.join(config.collection_dir, 'top1000.dev')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "\n",
    "set_seed(config.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset, TensorDataset, IterableDataset\n",
    "\n",
    "# Encode Dataset For Reranking\n",
    "class MarcoEncodeDataset(Dataset):\n",
    "    def __init__(self, collection_dir, tokenizer, mode='train', q_max_len=128, p_max_len=128):\n",
    "        self.collection_dir = collection_dir\n",
    "        self.tokenizer = tokenizer\n",
    "        self.mode = mode\n",
    "        self.q_max_len = q_max_len\n",
    "        self.p_max_len = p_max_len\n",
    "        # load data\n",
    "        passages_path = os.path.join(collection_dir, 'collection.tsv')\n",
    "        queries_path = os.path.join(collection_dir, f'queries.{mode}.tsv')\n",
    "        qrels_path = os.path.join(collection_dir, f'qrels.{mode}.tsv')\n",
    "        \n",
    "        self.passages = pd.read_csv(passages_path, sep='\\t', header=None, names=['pid', 'passage'], index_col='pid')\n",
    "        self.queries = pd.read_csv(queries_path, sep='\\t', header=None, names=['qid', 'query'], index_col='qid')\n",
    "        self.relations = pd.read_csv(qrels_path, sep='\\t', header=None, names=['qid', '0', 'pid', 'label'])\n",
    "        if self.mode == 'train':\n",
    "            top1000_path = os.path.join(collection_dir, f'top1000.{mode}.txt')\n",
    "            top1000_dict = utils.read_top1000(top1000_path)\n",
    "            self.top1000 = pd.DataFrame(list(top1000_dict.items()), columns=['qid', 'pid'])\n",
    "        else:\n",
    "            top1000_path = os.path.join(collection_dir, f'top1000.{mode}')\n",
    "            self.top1000 = pd.read_csv(top1000_path, sep='\\t', header=None, names=['qid', 'pid', 'query', 'passage'])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.top1000)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        x = self.top1000.iloc[idx]\n",
    "        query = self.queries.loc[x.qid].query\n",
    "        passage = self.passages.loc[x.pid].passage \n",
    "        label = 0 if self.relations.loc[(self.relations['qid'] == x.qid) & (self.relations['pid'] == x.pid)].empty else 1\n",
    "        \n",
    "        encode_query = self.tokenizer.encode_plus(\n",
    "            query,\n",
    "            max_length=self.q_max_len,\n",
    "            truncation='only_first',\n",
    "            return_token_type_ids=True,\n",
    "            return_attention_mask=True,\n",
    "            padding='max_length',\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        encoded_psg = self.tokenizer.encode_plus(\n",
    "            passage,\n",
    "            max_length=self.p_max_len,\n",
    "            truncation='only_first',\n",
    "            return_token_type_ids=True,\n",
    "            return_attention_mask=True,\n",
    "            padding='max_length',\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        encoded = {\n",
    "            'qid': x.qid,\n",
    "            'pid': x.pid,\n",
    "            'q_input_ids': encode_query['input_ids'], # query\n",
    "            'p_input_ids': encoded_psg['input_ids'], # passage\n",
    "            'q_attn_msk': encode_query['attention_mask'],\n",
    "            'p_attn_msk': encoded_psg['attention_mask'], \n",
    "            'label': torch.LongTensor([label]),\n",
    "        }\n",
    "        \n",
    "        return encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LLAMA_MODEL_LIST = [\n",
    "    'meta-llama/Llama-2-7b',\n",
    "    'meta-llam/Llama-2-7b-hf',\n",
    "    'meta-llama/Llama-2-7b-chat',\n",
    "    'meta-llama/Llama-2-7b-chat-hf',\n",
    "    'meta-llama/Llama-2-13b',\n",
    "    'meta-llama/Llama-2-13b-hf'\n",
    "    'meta-llama/Llama-2-13b-chat',\n",
    "    'meta-llama/Llama-2-13b-chat-hf',\n",
    "    'meta-llama/Llama-2-70b',\n",
    "    'meta-llama/Llama-2-70b-hf',\n",
    "    'meta-llama/Llama-2-70b-chat',\n",
    "    'meta-llama/Llama-2-70b-chat-hf',\n",
    "]\n",
    "\n",
    "n_gpu = os.cpu_count()\n",
    "\n",
    "class LlaMAReranker:\n",
    "    def __init__(self, model_name, use_cuda, batch_size=config.batch_size, n_gpu=n_gpu, mode='train'):\n",
    "        self.mode = mode\n",
    "        self.batch_size = batch_size\n",
    "        self.n_gpu = n_gpu\n",
    "\n",
    "        self.model = self.load_model(config.model_name, config.use_cuda)\n",
    "        self.model.eval()\n",
    "        self.tokenizer = self.load_tokenizer(config.model_name)\n",
    "        self.encode_dataset = MarcoEncodeDataset(config.collection_dir, self.tokenizer, mode=self.mode)\n",
    "        self.encode_dataloader = DataLoader(encode_dataset, batch_size=self.batch_size, num_workers=4*self.n_gpu)\n",
    "    \n",
    "    def load_model(self, model_name:str, use_cuda:bool):\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() & use_cuda else 'cpu')\n",
    "        model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float32).to(device)\n",
    "        model.config.use_cache=True\n",
    "\n",
    "        # for param in model.parameters():\n",
    "        #     param.requires_grad = False\n",
    "\n",
    "        return model\n",
    "    \n",
    "    def load_tokenizer(self, model_name:str):\n",
    "        tokenizer =  AutoTokenizer.from_pretrained(model_name)\n",
    "        return tokenizer\n",
    "    \n",
    "    # 그냥 일반 rerank (이때 assert q_max_len == p_max_len)\n",
    "    def rerank(self, encoded):\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() & use_cuda else 'cpu')\n",
    "        input_ids = torch.cat([encoded['q_input_ids'], encoded['p_input_ids']], dim=1).to(device)\n",
    "        attn_msk = torch.cat([encoded['q_attn_msk'], encoded['p_attn_msk']], dim=1).to(device)\n",
    "        inputs = {'input_ids': input_ids, 'attention_mask': attn_msk}\n",
    "        # q_input_ids = q_input_ids.to(device)\n",
    "        # p_input_ids = p_input_ids.to(device)\n",
    "        # p_len = len(p_input_ids)\n",
    "        with torch.no_grad():\n",
    "            logits = self.model(**inputs).logits\n",
    "            \n",
    "    \n",
    "    # def score(self):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llama_reranker = LlaMAReranker(LLAMA_MODEL_LIST[1], config.use_cuda)\n",
    "model = llama_reranker.model\n",
    "tokenizer = llama_reranker.tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "marco_encoded_train = MarcoEncodeDataset(config.collection_dir, tokenizer)\n",
    "# marco_encoded_dev = MarcoEncodeDataset(config.collection_dir, tokenizer, mode='dev')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_gpu = os.cpu_count()\n",
    "train_dataloader = DataLoader(marco_encoded_train, batch_size=config.batch_size, num_workers=4*n_gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = next(iter(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = torch.cat([a['query_input_ids'], a['psg_input_ids']], dim=1).to(device)\n",
    "attn_msk = torch.cat([a['query_attn_msk'], a['psg_attn_msk']], dim=1).to(device)\n",
    "inputs = {'input_ids': input_ids, 'attention_mask': attn_msk}\n",
    "with torch.no_grad():\n",
    "    logits = model(**inputs).logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://huggingface.co/models?search=gpt+neo\n",
    "GPT_PRETRAINED_MODEL_LIST = [\n",
    "    'EleutherAI/gpt-neo-125m',\n",
    "    'EleutherAI/gpt-neo-2.7B',\n",
    "    'EleutherAI/gpt-neo-1.3B'\n",
    "]\n",
    "\n",
    "class GPTReranker:\n",
    "    def __init__(self):\n",
    "        self.model = self.load_model(config.model_name, config.use_cuda)\n",
    "        self.tokenizer = self.load_tokenizer(config.model_name)\n",
    "        # tokenizer.pad_token = tokenizer.eos_token\n",
    "        self.model.eval()\n",
    "    \n",
    "    def load_model(self, model_name:str, use_cuda:bool):\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() & use_cuda else 'cpu')\n",
    "        model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float32).to(device)\n",
    "        model.config.use_cache=True\n",
    "        return model\n",
    "    \n",
    "    def load_tokenizer(self, model_name:str):\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        return tokenizer\n",
    "    \n",
    "    def _get_prompt(self, query)\n",
    "    \n",
    "    def rerank(self, query, texts):\n",
    "        prompt =  f\"Please generate a query based on the following passage: {texts}\"\n",
    "\n",
    "# model = AutoModelForCausalLM.from_pretrained('EleutherAI/gpt-neo-125m')\n",
    "# tokenizer = AutoTokenizer.from_pretrained('EleutherAI/gpt-neo-125m')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rerank",
   "language": "python",
   "name": "rerank"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "vscode": {
   "interpreter": {
    "hash": "b6d079cb6e5a2b7f1ce0a2a601ae4ad26fa575f288e2b21025e8b535c649f8dd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
