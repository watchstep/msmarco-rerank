{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import json\n",
    "import requests\n",
    "import io\n",
    "from zipfile import ZipFile\n",
    "import argparse\n",
    "import multiprocessing\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "\n",
    "import pyserini\n",
    "from pyserini.search import SimpleSearcher\n",
    "from pyserini.dsearch import SimpleDenseSearcher\n",
    "\n",
    "import transformers\n",
    "# from transformers import set_seed\n",
    "# set_seed(42)\n",
    "\n",
    "from peft import LoraConfig\n",
    "from transformers import (AutoTokenizer, \n",
    "                          AutoModelForCausalLM, \n",
    "                          BitsAndBytesConfig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(description='Reranking with OPT & NFCorpus')\n",
    "\n",
    "parser.add_argument('--model_name', type=str, default='facebook/opt-125m')\n",
    "parser.add_argument('--dataset', type=str, default='nfcorpus')\n",
    "parser.add_argument('--data_path', type=str, default='./collections/')\n",
    "parser.add_argument('--seed',type=int, default=42)\n",
    "parser.add_argument('--batch_size', type=int, default=256)\n",
    "parser.add_argument('--max_len', type=int, default=40)\n",
    "parser.add_argument('--lr', type=float, default=1e-5)\n",
    "parser.add_argument('--max_epochs', type=int, default=10)\n",
    "parser.add_argument('--use_cuda', type=bool, default=False)\n",
    "parser.add_argument('--k', type=int, default=10, help='top k')\n",
    "parser.add_argument('--k1', type=float, default=1.5, help='BM25 parameter')\n",
    "parser.add_argument('--b', type=float, default=0.75, help='BM25 parameter')\n",
    "\n",
    "parser.add_argument\n",
    "\n",
    "config = parser.parse_args([])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare NFCorpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = os.path.join(config.data_path, config.dataset)\n",
    "nfcorpus_url = 'https://public.ukp.informatik.tu-darmstadt.de/thakur/BEIR/datasets/nfcorpus.zip'\n",
    "tsv_path = os.path.join(dataset_path, config.dataset, 'queries.tsv')\n",
    "queries_jsonl_path = os.path.join(dataset_path, config.dataset, 'queries.jsonl')\n",
    "corpus_jsonl_path = os.path.join(dataset_path, config.dataset, 'corpus.jsonl')\n",
    "pyserini_jsonl_path = os.path.join(dataset_path, 'pyserini-corpus', 'corpus.jsonl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download data\n",
    "if not os.path.exists(dataset_path):\n",
    "    os.mkdir(dataset_path)\n",
    "\n",
    "response = requests.get(nfcorpus_url, stream=True)\n",
    "file = ZipFile(io.BytesIO(response.content))\n",
    "file.extractall(path=dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(tsv_path, 'w') as out:\n",
    "    with open(queries_jsonl_path, 'r') as f:\n",
    "        for line in f:\n",
    "            l = json.loads(line)\n",
    "            out.write(l['_id'] + '\\t' + l['text'] + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/castorini/pyserini/blob/e371ed3661e90db6b797290493d973cb6c089c43/docs/conceptual-framework2.md\n",
    "with open(pyserini_jsonl_path, 'w') as out:\n",
    "    with open(corpus_jsonl_path, 'r') as f:\n",
    "        for line in f:\n",
    "            l = json.loads(line)\n",
    "            s = json.dumps({'id': l['_id'], 'contents': l['title'] + ' ' + l['text']})\n",
    "            out.write(s + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sparse Indexing & BM25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from pyserini.search.lucene import LuceneSearcher \n",
    "\n",
    "class BM25Retriever:\n",
    "    def __init__(self, jsonl_path, index_path, k1=1.5, b=0.75):\n",
    "        self.jsonl_path = jsonl_path\n",
    "        if not os.path.exists(index_path):\n",
    "            self.build_sparse_index(jsonl_path, index_path)\n",
    "        self.searcher = LuceneSearcher(index_path)\n",
    "        self.searcher.set_bm25(k1=k1, b=b)\n",
    "        # self.searcher.set_language()\n",
    "    \n",
    "    def build_sparse_index(self, jsonl_path, index_path):\n",
    "        execute_code = os.system('python -m pyserini.index.lucene ' + \n",
    "                                 '--collection JsonCollection ' +\n",
    "                                 f'--input {jsonl_path} ' +\n",
    "                                 f'--index {index_path} ' +\n",
    "                                 '--generator DefaultLuceneDocumentGenerator ' +\n",
    "                                 '--threads 1 --storeRaw')\n",
    "        if execute_code != 0:\n",
    "            raise Exception('Indexing Failed!')\n",
    "        else:\n",
    "            print('Indexing Success!')\n",
    "            \n",
    "    def _get_results(self, qid, hits:List):\n",
    "        results = []\n",
    "        \n",
    "        for i, hit in enumerate(hits):\n",
    "            docid = hit.docid\n",
    "            content = json.loads(hits[i].raw)['contents']\n",
    "            bm25_score = hit.score\n",
    "            result = {'rank': i+1,\n",
    "                      'qid': qid,\n",
    "                      'docid': docid, \n",
    "                      'score': bm25_score,\n",
    "                      'content': content}\n",
    "            results.append(result)\n",
    "            \n",
    "        return results\n",
    "    \n",
    "    def search(self, qid, query_text:str, k:int=10):\n",
    "        search_results = {}\n",
    "        hits = self.searcher.search(query_text, k=k,)\n",
    "        search_results['query'] = query_text\n",
    "        search_results['hits']  = self._get_results(qid, hits)\n",
    "        \n",
    "        return search_results\n",
    "    \n",
    "    def batch_search(self, qids, query_texts: List[str], k:int=10):\n",
    "        query_dict = dict(zip(qids, query_texts))\n",
    "        batch_hits = self.searcher.batch_search(query_texts, qids, k=k, threads=multiprocessing.cpu_count())\n",
    "        bsearch_results = []\n",
    "        bsearch_items = {}\n",
    "\n",
    "        for qid, hits in batch_hits.items():\n",
    "            bsearch_items['query'] = query_dict[qid]\n",
    "            bsearch_items['hits'] = self._get_results(qid, hits)\n",
    "            bsearch_results.append(bsearch_items)\n",
    "            bsearch_items = {} \n",
    "       \n",
    "        return bsearch_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_path = os.path.join('./indexes', 'lucene-index.nfcorpus')\n",
    "bm25_retriever = BM25Retriever('collections/nfcorpus/pyserini-corpus/', index_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_jsonl(jsonl_path:str):\n",
    "    with open(jsonl_path, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        result = [json.loads(line) for line in lines]\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = read_jsonl(queries_jsonl_path)\n",
    "results = bm25_retriever.batch_search(qids=[query['_id'] for query in queries], query_texts=[query['text'] for query in queries])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "json.dump(results, open('./retrieved/bm25-nfcorpus.jsonl','w'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rerank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List \n",
    "\n",
    "\n",
    "class GPTReranker:\n",
    "    def __init__(self, model_name, k, use_cuda, max_len):\n",
    "        self.model = self.load_model(model_name, use_cuda)\n",
    "        self.tokenizer = self.load_tokenizer(model_name)\n",
    "        self.max_len = max_len\n",
    "        \n",
    "        self.model.eval()\n",
    "    \n",
    "    def load_model(self, model_name:str, use_cuda:bool):\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() & use_cuda else 'cpu')\n",
    "        model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16).to(device)\n",
    "        model.config.use_cache=True\n",
    "        return model\n",
    "    \n",
    "    def load_tokenizer(self, model_name:str):\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        return tokenizer\n",
    "    \n",
    "    def score(self, dataset):\n",
    "        encodings = self.tokenizer(dataset, add_special_tokens=False, return_tensors='pt')\n",
    "        \n",
    "        dataset_len = encodings.input_ids.size(1)\n",
    "        \n",
    "    \n",
    "    # def rerank(self, query: Query, texts: List[Text]) -> List[Text]:\n",
    "    #     for text in texts:\n",
    "    #         result = self.tokenizer(query.text)\n",
    "    #         input_ids\n",
    "    #         attn_mask\n",
    "            \n",
    "        \n",
    "            \n",
    "        \n",
    "    #     return super().rerank(query, texts)\n",
    "    \n",
    "    # # similarity score\n",
    "    # # deft score(self, input_ids, item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_reranker = GPTReranker(config.model_name, config.use_cuda, max_len=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "qids = [query['_id'] for query in queries]\n",
    "query_texts = [query['text'] for query in queries]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 7205, 33178, 12, 246]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "instance = gpt_reranker.tokenizer(qids)\n",
    "instance['input_ids'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    2,  7205, 33178,    12,   246]])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids = torch.tensor(instance['input_ids'][0]).unsqueeze(0)\n",
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'token_type_ids'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\cathy\\OneDrive\\바탕 화면\\rerank\\rerank_nfcorpus.ipynb Cell 22\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/cathy/OneDrive/%EB%B0%94%ED%83%95%20%ED%99%94%EB%A9%B4/rerank/rerank_nfcorpus.ipynb#Y104sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m token_type_ids \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor(instance[\u001b[39m'\u001b[39;49m\u001b[39mtoken_type_ids\u001b[39;49m\u001b[39m'\u001b[39;49m][\u001b[39m0\u001b[39m])\u001b[39m.\u001b[39munsqueeze(\u001b[39m0\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\cathy\\.conda\\envs\\rerank\\lib\\site-packages\\transformers\\tokenization_utils_base.py:253\u001b[0m, in \u001b[0;36mBatchEncoding.__getitem__\u001b[1;34m(self, item)\u001b[0m\n\u001b[0;32m    243\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    244\u001b[0m \u001b[39mIf the key is a string, returns the value of the dict associated to `key` ('input_ids', 'attention_mask',\u001b[39;00m\n\u001b[0;32m    245\u001b[0m \u001b[39metc.).\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    250\u001b[0m \u001b[39mwith the constraint of slice.\u001b[39;00m\n\u001b[0;32m    251\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    252\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(item, \u001b[39mstr\u001b[39m):\n\u001b[1;32m--> 253\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdata[item]\n\u001b[0;32m    254\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_encodings \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    255\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_encodings[item]\n",
      "\u001b[1;31mKeyError\u001b[0m: 'token_type_ids'"
     ]
    }
   ],
   "source": [
    "token_type_ids = torch.tensor(instance['token_type_ids'][0]).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_inputs(item, device, tokenizer):\n",
    "    input_ids = torch.tensor(['input_ids'], device=device).unsqueeze(0)\n",
    "    input_ids = tokenizer.decode()\n",
    "    input_ids, \n",
    "    token_type \n",
    "    attn_mask\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def normalize_answer(s):\n",
    "    \"\"\"\n",
    "    Taken from the official evaluation script for v1.1 of the SQuAD dataset.\n",
    "    Lower text and remove punctuation, articles and extra whitespace.\n",
    "    \"\"\"\n",
    "\n",
    "    def remove_articles(text):\n",
    "        return re.sub(r\"\\b(a|an|the)\\b\", \" \", text)\n",
    "\n",
    "    def white_space_fix(text):\n",
    "        return \" \".join(text.split())\n",
    "\n",
    "    def remove_punc(text):\n",
    "        exclude = set(string.punctuation)\n",
    "        return \"\".join(ch for ch in text if ch not in exclude)\n",
    "\n",
    "    def lower(text):\n",
    "        return text.lower()\n",
    "\n",
    "    return white_space_fix(remove_articles(remove_punc(lower(s))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# top_results = torch.topk(scores, k=5).indices \n",
    "# reranked_corpus = [corpus[i] for i in top_results] \n",
    "\n",
    "# scored_articles = zip(articles, cosine_similarities)\n",
    "\n",
    "# # Sort articles by cosine similarity\n",
    "# sorted_articles = sorted(scored_articles, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# scores = []\n",
    "# https://github.com/amazon-science/datatuner/blob/f70369659e1c58e6ddb44d6db467978679dbdd3c/src/datatuner/lm/reranker.py#L5 \n",
    "\n",
    "# sorted(initial_results, key=lambda x: x.score or 0.0, reverse=True)[\n",
    "        #     : self.top_n\n",
    "        # ]\n",
    "\n",
    "# reranked = reranker.rerank(query, texts)\n",
    "#     reranked.sort(key=lambda x: x.score, reverse=True)\n",
    "#     d['documents'] = [text.metadata for text in reranked]\n",
    "#   results.append(item)\n",
    "# \n",
    "\n",
    "# json.dump(results, open(\"iirc_reranked.json\",'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(config.model)\n",
    "model = AutoModelForCausalLM.from_pretrained(config.model,\n",
    "                                             quantization_config=quantization_config,\n",
    "                                             trust_remote_code=True,)\n",
    "\n",
    "model.config.use_cache=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.05,\n",
    "    target_modules = [\"q_proj\", \"v_proj\"],\n",
    "    inference_mode=False,\n",
    "    bias='none',\n",
    "    task_type='CAUSAL_LM',\n",
    ")\n",
    "\n",
    "model = get"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rerank",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
