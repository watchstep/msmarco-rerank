{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import json\n",
    "import requests\n",
    "import io\n",
    "from zipfile import ZipFile\n",
    "import argparse\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "\n",
    "import pyserini\n",
    "from pyserini.search import SimpleSearcher\n",
    "from pyserini.dsearch import SimpleDenseSearcher\n",
    "\n",
    "import transformers\n",
    "# from transformers import set_seed\n",
    "# set_seed(42)\n",
    "\n",
    "from peft import LoraConfig\n",
    "from transformers import (AutoTokenizer, \n",
    "                          AutoModelForCausalLM, \n",
    "                          BitsAndBytesConfig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(description='Reranking with LLaMA2')\n",
    "\n",
    "parser.add_argument('--model_name', type=str, default='Llama-2-7b-hf')\n",
    "parser.add_argument('--dataset', type=str, default='nfcorpus')\n",
    "parser.add_argument('--data_path', type=str, default='./collections/')\n",
    "parser.add_argument('--seed',type=int, default=42)\n",
    "parser.add_argument('--batch_size', type=int, default=256)\n",
    "parser.add_argument('--max_len', type=int, default=40)\n",
    "parser.add_argument('--lr', type=float, default=1e-5)\n",
    "parser.add_argument('--max_epochs', type=int, default=10)\n",
    "parser.add_argument('--use_cuda', type=bool, default=False)\n",
    "parser.add_argument('--k', type=int, default=10, help='top k')\n",
    "parser.add_argument('--k1', type=float, default=1.5, help='BM25 parameter')\n",
    "parser.add_argument('--b', type=float, default=0.75, help='BM25 parameter')\n",
    "\n",
    "parser.add_argument\n",
    "\n",
    "config = parser.parse_args([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = os.path.join(config.data_path)\n",
    "targz_path = os.path.join(dataset_path, 'collectionandqueries.tar.gz')\n",
    "nfcorpus_url = 'https://public.ukp.informatik.tu-darmstadt.de/thakur/BEIR/datasets/nfcorpus.zip'\n",
    "tsv_path = os.path.join(dataset_path, config.dataset, 'queries.tsv')\n",
    "jsonl_path = os.path.join(dataset_path, config.dataset, 'queries.jsonl')\n",
    "index_path = os.path.join('./indexes', 'lucene-index-nfcorpus')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download data\n",
    "if not os.path.exists(dataset_path):\n",
    "    os.mkdir(dataset_path)\n",
    "\n",
    "response = requests.get(nfcorpus_url, stream=True)\n",
    "file = ZipFile(io.BytesIO(response.content))\n",
    "file.extractall(path=dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(tsv_path, 'w') as out:\n",
    "    with open(jsonl_path, 'r') as f:\n",
    "        for line in f:\n",
    "            l = json.loads(line)\n",
    "            out.write(l['_id'] + '\\t' + l['text'] + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyserini.search.lucene import LuceneSearcher\n",
    "\n",
    "# https://github.com/castorini/pyserini/blob/master/docs/prebuilt-indexes.md\n",
    "searcher = LuceneSearcher.from_prebuilt_index('beir-v1.0.0-nfcorpus.flat')\n",
    "\n",
    "hits = searcher.search('Hello')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from pyserini.search.lucene import LuceneSearcher \n",
    "\n",
    "# Indexer # Retriever # BaseRetriever 만들고 BM25, ANCE, Hybrid\n",
    "# build_sparse_index \n",
    "# build_dense_index\n",
    "# ssearch, dsearch, hsearch\n",
    "\n",
    "class BM25Retriever:\n",
    "    def __init__(self, jsonl_path, index_path, k1=1.5, b=0.75):\n",
    "        self.jsonl_path = jsonl_path\n",
    "        if not os.path.exists(index_path):\n",
    "            self.build_sparse_index(jsonl_path, index_path)\n",
    "        self.searcher = LuceneSearcher(index_path) # searcher = SimpleSearcher.from_prebuilt_index('msmarco-passage')\n",
    "        self.searcher.set_bm25(k1=k1, b=b)\n",
    "        # self.searcher.set_language()\n",
    "    \n",
    "    def build_sparse_index(self, jsonl_path, index_path): # 나중에 dense, hybird하기 위해 build_dense_index 만들고 새로운 class 만들기\n",
    "        execute_code = os.system('python -m pyserini.index.lucene ' + \n",
    "                                 '--collection JsonCollection ' +\n",
    "                                 f'--input {jsonl_path} ' +\n",
    "                                 f'--index {index_path} ' +\n",
    "                                 '--generator DefaultLuceneDocumentGenerator ' +\n",
    "                                 '--threads 1 --storeRaw')\n",
    "        if execute_code != 0:\n",
    "            raise Exception('Indexing Failed!')\n",
    "        else:\n",
    "            print('Indexing Success!')\n",
    "            \n",
    "    def _get_results(self, qid, hits:List):\n",
    "        results = []\n",
    "        \n",
    "        for i, hit in enumerate(hits):\n",
    "            docid = hit.docid\n",
    "            content = json.loads(hits[i].raw)['contents']\n",
    "            bm25_score = hit.score\n",
    "            result = {'rank': i,\n",
    "                      'qid': qid,\n",
    "                      'docid': docid, \n",
    "                      'bm25_score': bm25_score,\n",
    "                      'content': content}\n",
    "            results.append(result)\n",
    "            \n",
    "        return results\n",
    "    \n",
    "    def search(self, qid, query:str, k:int=10):\n",
    "        hits = self.searcher.search(query, k=k)\n",
    "        search_results  = self._get_results(qid, hits)\n",
    "        \n",
    "        return search_results\n",
    "    \n",
    "    def batch_search(self, queries: List[str], qids: List[str], k:int=10):\n",
    "        batch_hits = self.searcher.batch_search(queries, qids, k=k)\n",
    "        bsearch_results = {}\n",
    "        \n",
    "        for qid, hits in batch_hits.items():\n",
    "            bsearch_results[qid] = self._get_results(qid, hits)\n",
    "        \n",
    "        return bsearch_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jsonl_path = os.path.join(dataset_path, config.dataset, 'corpus.jsonl')\n",
    "index_path = os.path.join('./indexes', 'lucene-index-nfcorpus')\n",
    "\n",
    "bm25_retriever = BM25Retriever(jsonl_path, index_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bm25_retriever.search('How Contaminated Are Our Children?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List \n",
    "from base import Reranker, Query, Text # pygaggle\n",
    "\n",
    "# https://github.com/informagi/EMBERT/blob/f89efeeeef53d4dc9e2cc1f2b547aa34aa4f7945/Code/pygaggle/rerank/transformer.py\n",
    "class LLaMAReranker(Reranker):\n",
    "    def __init__(self, model_name, max_len, use_cuda):\n",
    "        self.model = self.load_model(model_name, use_cuda)\n",
    "        self.tokenizer = self.load_tokenizer(model_name)\n",
    "        self.max_len = max_len\n",
    "    \n",
    "    def load_model(self, model_name:str, use_cuda:bool):\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() & use_cuda else 'cpu')\n",
    "        model = AutoModelForCausalLM.from_pretrained(f'meta-llama/{model_name}', torch_dtype=torch.float16).to(device)\n",
    "        return model\n",
    "    \n",
    "    def load_tokenizer(self, model_name:str):\n",
    "        tokenizer = AutoTokenizer.from_pretrained(f'meta-llama/{model_name}')\n",
    "        return tokenizer\n",
    "        \n",
    "    \n",
    "    def rerank(self, query: Query, texts: List[Text]) -> List[Text]:\n",
    "        for text in texts:\n",
    "            result = self.tokenizer(query.text)\n",
    "            input_ids\n",
    "            attn_mask\n",
    "            \n",
    "        \n",
    "            \n",
    "        \n",
    "        return super().rerank(query, texts)\n",
    "    \n",
    "    # similarity score\n",
    "    # deft score(self, input_ids, item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_inputs(item, device, tokenizer):\n",
    "    input_ids = torch.tensor(['input_ids'], device=device).unsqueeze(0)\n",
    "    input_ids = tokenizer.decode()\n",
    "    input_ids, \n",
    "    token_type \n",
    "    attn_mask\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# top_results = torch.topk(scores, k=5).indices \n",
    "# reranked_corpus = [corpus[i] for i in top_results] \n",
    "\n",
    "# scored_articles = zip(articles, cosine_similarities)\n",
    "\n",
    "# # Sort articles by cosine similarity\n",
    "# sorted_articles = sorted(scored_articles, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# scores = []\n",
    "# https://github.com/amazon-science/datatuner/blob/f70369659e1c58e6ddb44d6db467978679dbdd3c/src/datatuner/lm/reranker.py#L5 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(config.model)\n",
    "model = AutoModelForCausalLM.from_pretrained(config.model,\n",
    "                                             quantization_config=quantization_config,\n",
    "                                             trust_remote_code=True,)\n",
    "\n",
    "model.config.use_cache=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.05,\n",
    "    target_modules = [\"q_proj\", \"v_proj\"],\n",
    "    inference_mode=False,\n",
    "    bias='none',\n",
    "    task_type='CAUSAL_LM',\n",
    ")\n",
    "\n",
    "model = get"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rerank",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
