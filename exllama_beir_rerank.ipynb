{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import argparse\n",
    "import json\n",
    "\n",
    "p_prompt = f\"\"\"\n",
    "Passage: <P> Given the provided passage, generate 3 similar passages on related topic: <T>\n",
    "\"\"\"\n",
    "\n",
    "q_prompt = \"\"\"\n",
    "<P> Review the given passages and answer a specific and detailed query. {'Query: Your query here.'}”\n",
    "\"\"\"\n",
    "\n",
    "parser = argparse.ArgumentParser(description = \"Prompting with ExLlamaV2\")\n",
    "parser.add_argument(\"--dataset\", type = str, default ='nq')\n",
    "parser.add_argument(\"--seed\", type = int, default = 2023)\n",
    "parser.add_argument(\"--topk\", type=int, default=10)\n",
    "parser.add_argument(\"--model_dir\", type=str, default=\"/root/Mistral-7B-instruct-exl2\")\n",
    "parser.add_argument(\"--p_prompt\", type=str, default=p_prompt)\n",
    "parser.add_argument(\"--q_prompt\", type=str, default=q_prompt)\n",
    "\n",
    "args = parser.parse_args([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/rerank/lib/python3.10/site-packages/beir/util.py:2: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm\n",
      "./data/nq.zip: 100%|██████████| 475M/475M [00:49<00:00, 10.0MiB/s]  \n",
      "100%|██████████| 2681468/2681468 [00:12<00:00, 213131.69it/s]\n"
     ]
    }
   ],
   "source": [
    "from beir import util\n",
    "from beir.datasets.data_loader import GenericDataLoader\n",
    "\n",
    "url = f\"https://public.ukp.informatik.tu-darmstadt.de/thakur/BEIR/datasets/{args.dataset}.zip\"\n",
    "data_path = util.download_and_unzip(url, './data/')\n",
    "\n",
    "corpus, queries, qrels = GenericDataLoader(data_folder=data_path).load(split=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Download & Setup ElasticSearch\n",
    "\n",
    "# %%bash\n",
    "\n",
    "# wget -q https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-oss-7.9.2-linux-x86_64.tar.gz\n",
    "# wget -q https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-oss-7.9.2-linux-x86_64.tar.gz.sha512\n",
    "# tar -xzf elasticsearch-oss-7.9.2-linux-x86_64.tar.gz\n",
    "# sudo chown -R daemon:daemon elasticsearch-7.9.2/\n",
    "# shasum -a 512 -c elasticsearch-oss-7.9.2-linux-x86_64.tar.gz.sha512\n",
    "\n",
    "# %%bash --bg\n",
    "\n",
    "# sudo -H -u daemon elasticsearch-7.9.2/bin/elasticsearch\n",
    "\n",
    "# %%bash\n",
    "\n",
    "# ps -ef | grep elasticsearch\n",
    "\n",
    "# %%bash\n",
    "\n",
    "# curl -sX GET \"localhost:9200/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from beir.retrieval.search.lexical import BM25Search as BM25\n",
    "from beir.retrieval.evaluation import EvaluateRetrieval\n",
    "\n",
    "hostname = \"localhost\"\n",
    "index_name = args.dataset\n",
    "initialize = True\n",
    "\n",
    "model = BM25(index_name=index_name, hostname=hostname, initialize=initialize)\n",
    "retriever = EvaluateRetrieval(model)\n",
    "\n",
    "# results = retriever.retrieve(corpus, queries)\n",
    "# # ndcg, _map, recall, precision = retriever.evaluate(qrels, results, retriever.k_values)\n",
    "# output = {'retrieval': EvaluateRetrieval.evaluate(qrels, results, retriever.k_values)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_json(results, file_name:str):\n",
    "    json_path = f'./output/{args.dataset}_{file_name}.json'\n",
    "    with open(json_path, 'w') as f:\n",
    "        json.dump(results, f, indent=4)\n",
    "\n",
    "def read_json(json_path:str):\n",
    "    with open(json_path, 'r') as f:\n",
    "        data = f.read()\n",
    "        output = json.loads(data)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_json(results, 'bm25_es_retrieved')\n",
    "save_json(output, 'eval_output')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = read_json(f'./output/{args.dataset}_bm25_es_retrieved.json')\n",
    "output = read_json(f'./output/{args.dataset}_eval_output.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from exllamav2 import(\n",
    "    ExLlamaV2,\n",
    "    ExLlamaV2Config,\n",
    "    ExLlamaV2Cache,\n",
    "    ExLlamaV2Tokenizer,\n",
    ")\n",
    "\n",
    "from exllamav2.generator import (\n",
    "    ExLlamaV2BaseGenerator,\n",
    "    ExLlamaV2Sampler\n",
    ")\n",
    "\n",
    "# from beir.reranking import Rerank\n",
    "from typing import List, Tuple\n",
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/beir-cellar/beir/blob/main/beir/reranking/rerank.py\n",
    "from typing import Dict, List\n",
    "\n",
    "class Rerank:\n",
    "    def __init__(self, model, batch_size: int = 128, **kwargs):\n",
    "        self.model = model\n",
    "        self.batch_size = batch_size\n",
    "        self.rerank_results = {}\n",
    "    \n",
    "    def rerank(self,\n",
    "               corpus: Dict[str, Dict[str, str]],\n",
    "               queries: Dict[str, str],\n",
    "               results: Dict[str, Dict[str, float]],\n",
    "               top_k: int) -> Dict[str, Dict[str, float]]:\n",
    "        \n",
    "        sentence_pairs, pair_ids = [], []\n",
    "        \n",
    "        for query_id in results:\n",
    "            if len(results[query_id]) > top_k:\n",
    "                for (doc_id, _) in sorted(results[query_id].items(), key=lambda item: item[1], reverse=True)[:top_k]:\n",
    "                    pair_ids.append([query_id, doc_id])\n",
    "                    corpus_text = {'title': corpus[doc_id].get(\"title\", \"\").strip(), 'text': corpus[doc_id].get(\"text\", \"\").strip()}\n",
    "                    sentence_pairs.append([queries[query_id], corpus_text])\n",
    "                else:\n",
    "                    for doc_id in results[query_id]:\n",
    "                        pair_ids.append([query_id, doc_id])\n",
    "                        corpus_text = {'title': corpus[doc_id].get(\"title\", \"\").strip(), 'text': corpus[doc_id].get(\"text\", \"\").strip()}\n",
    "                        # corpus_text = (corpus[doc_id].get(\"title\", \"\") + \" \" + corpus[doc_id].get(\"text\", \"\")).strip()\n",
    "                        sentence_pairs.append([queries[query_id], corpus_text])\n",
    "        \n",
    "        #### Starting to Rerank using cross-attention\n",
    "        rerank_scores = [float(score) for score in self.model.predict(sentence_pairs, batch_size=self.batch_size)]\n",
    "        \n",
    "        #### Reranking results\n",
    "        self.rerank_results = {query_id: {} for query_id in results}\n",
    "        for pair, score in zip(pair_ids, rerank_scores):\n",
    "            query_id, doc_id = pair[0], pair[1]\n",
    "            self.rerank_results[query_id][doc_id] = score\n",
    "        \n",
    "        return self.rerank_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/beir-cellar/beir/wiki/Evaluate-your-custom-model\n",
    "# https://github.com/Muennighoff/sgpt/blob/main/crossencoder/beir/sgptce.py\n",
    "# https://github.com/beir-cellar/beir/blob/main/examples/retrieval/evaluation/reranking/evaluate_bm25_ce_reranking.py\n",
    "# https://github.com/beir-cellar/beir/blob/main/beir/reranking/models/mono_t5.py\n",
    "\n",
    "class ExLlamaV2Reranker:\n",
    "    def __init__(self, p_prompt=args.p_prompt, q_prompt=args.q_prompt, model_dir=args.model_dir, seed=args.seed, max_new_tokens=400, **kwargs):\n",
    "        self.p_prompt = p_prompt\n",
    "        self.q_prompt = q_prompt\n",
    "        self.seed = seed\n",
    "        self.max_new_tokens = max_new_tokens\n",
    "        \n",
    "        self.config = ExLlamaV2Config()\n",
    "        self.config.model_dir = model_dir\n",
    "        self.config.prepare()\n",
    "        \n",
    "        self.model = ExLlamaV2(self.config)\n",
    "        if not self.model.loaded:\n",
    "            self.cache = ExLlamaV2Cache(self.model, lazy = True)\n",
    "            self.model.load_autosplit(self.cache)\n",
    "            \n",
    "        self.tokenizer = ExLlamaV2Tokenizer(self.config)\n",
    "        self.generator = ExLlamaV2BaseGenerator(self.model, self.cache, self.tokenizer)\n",
    "        self.generator.warmup()\n",
    "        \n",
    "        self.settings = ExLlamaV2Sampler.Settings()\n",
    "        self.settings.temperature = 0.6\n",
    "        self.settings.top_k = 50\n",
    "        self.settings.top_p = 0.9\n",
    "        self.settings.token_repetition_penalty = 1.15\n",
    "        self.settings.disallow_tokens(self.tokenizer, [self.tokenizer.eos_token_id])\n",
    "    \n",
    "    def predict(self, sentences: List[Tuple[str,str]], batch_size: int, **kwags) -> List[float]:\n",
    "        scores = []\n",
    "        for query, psg in sentences:\n",
    "            self.p_prompt = self.p_prompt.replace('<P>', psg['text']).replace('<T>', psg['title']).strip()\n",
    "            generated = self.generator.generate_simple(self.p_prompt, self.settings, self.max_new_tokens, seed=self.seed)\n",
    "            self.q_prompt = self.q_prompt.replace('<P>', generated).strip()\n",
    "            with torch.inference_mode():\n",
    "                input_ids = self.tokenizer.encode(self.q_prompt)\n",
    "                # input_ids = input_ids.shape[-1]\n",
    "                # self.cache.current_seq_len = 0\n",
    "                logits = self.model.forward(input_ids[:, -1:], self.cache)\n",
    "                logits = logits[:, :-1, :]\n",
    "                logits = logits.float() + 1e-10\n",
    "                log_probs = F.log_softmax(logits, dim=-1)\n",
    "            scores.append(log_probs)\n",
    "        \n",
    "        assert len(scores) == len(sentences)\n",
    "        return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/root/workspace/exllama_beir_rerank.ipynb Cell 11\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Brerank/root/workspace/exllama_beir_rerank.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m reranker \u001b[39m=\u001b[39m Rerank(ExLlamaV2Reranker(), batch_size\u001b[39m=\u001b[39m\u001b[39m128\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Brerank/root/workspace/exllama_beir_rerank.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m rerank_results \u001b[39m=\u001b[39m reranker\u001b[39m.\u001b[39;49mrerank(corpus, queries, results, top_k\u001b[39m=\u001b[39;49m\u001b[39m100\u001b[39;49m)\n",
      "\u001b[1;32m/root/workspace/exllama_beir_rerank.ipynb Cell 11\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Brerank/root/workspace/exllama_beir_rerank.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=28'>29</a>\u001b[0m                 sentence_pairs\u001b[39m.\u001b[39mappend([queries[query_id], corpus_text])\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Brerank/root/workspace/exllama_beir_rerank.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=30'>31</a>\u001b[0m \u001b[39m#### Starting to Rerank using cross-attention\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Brerank/root/workspace/exllama_beir_rerank.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=31'>32</a>\u001b[0m rerank_scores \u001b[39m=\u001b[39m [\u001b[39mfloat\u001b[39m(score) \u001b[39mfor\u001b[39;00m score \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49mpredict(sentence_pairs, batch_size\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbatch_size)]\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Brerank/root/workspace/exllama_beir_rerank.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=33'>34</a>\u001b[0m \u001b[39m#### Reranking results\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Brerank/root/workspace/exllama_beir_rerank.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=34'>35</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrerank_results \u001b[39m=\u001b[39m {query_id: {} \u001b[39mfor\u001b[39;00m query_id \u001b[39min\u001b[39;00m results}\n",
      "\u001b[1;32m/root/workspace/exllama_beir_rerank.ipynb Cell 11\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Brerank/root/workspace/exllama_beir_rerank.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=34'>35</a>\u001b[0m \u001b[39mfor\u001b[39;00m query, psg \u001b[39min\u001b[39;00m sentences:\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Brerank/root/workspace/exllama_beir_rerank.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=35'>36</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mp_prompt \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mp_prompt\u001b[39m.\u001b[39mreplace(\u001b[39m'\u001b[39m\u001b[39m<P>\u001b[39m\u001b[39m'\u001b[39m, psg[\u001b[39m'\u001b[39m\u001b[39mtext\u001b[39m\u001b[39m'\u001b[39m])\u001b[39m.\u001b[39mreplace(\u001b[39m'\u001b[39m\u001b[39m<T>\u001b[39m\u001b[39m'\u001b[39m, psg[\u001b[39m'\u001b[39m\u001b[39mtitle\u001b[39m\u001b[39m'\u001b[39m])\u001b[39m.\u001b[39mstrip()\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Brerank/root/workspace/exllama_beir_rerank.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=36'>37</a>\u001b[0m     generated \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgenerator\u001b[39m.\u001b[39;49mgenerate_simple(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mp_prompt, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msettings, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_new_tokens, seed\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mseed)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Brerank/root/workspace/exllama_beir_rerank.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=37'>38</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mq_prompt \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mq_prompt\u001b[39m.\u001b[39mreplace(\u001b[39m'\u001b[39m\u001b[39m<P>\u001b[39m\u001b[39m'\u001b[39m, generated)\u001b[39m.\u001b[39mstrip()\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Brerank/root/workspace/exllama_beir_rerank.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=38'>39</a>\u001b[0m     \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39minference_mode():\n",
      "File \u001b[0;32m~/miniconda3/envs/rerank/lib/python3.10/site-packages/exllamav2/generator/base.py:107\u001b[0m, in \u001b[0;36mExLlamaV2BaseGenerator.generate_simple\u001b[0;34m(self, prompt, gen_settings, num_tokens, seed, token_healing, encode_special_tokens, decode_special_tokens, loras, stop_token)\u001b[0m\n\u001b[1;32m    103\u001b[0m batch_eos \u001b[39m=\u001b[39m [\u001b[39mFalse\u001b[39;00m] \u001b[39m*\u001b[39m batch_size\n\u001b[1;32m    105\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_tokens):\n\u001b[0;32m--> 107\u001b[0m     logits \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49mforward(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msequence_ids[:, \u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m:], \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcache, input_mask \u001b[39m=\u001b[39;49m mask, loras \u001b[39m=\u001b[39;49m loras)\u001b[39m.\u001b[39mfloat()\u001b[39m.\u001b[39mcpu()\n\u001b[1;32m    108\u001b[0m     token, _, _ \u001b[39m=\u001b[39m ExLlamaV2Sampler\u001b[39m.\u001b[39msample(logits, gen_settings, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msequence_ids, random\u001b[39m.\u001b[39mrandom(), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtokenizer, prefix_token \u001b[39m=\u001b[39m unhealed_token)\n\u001b[1;32m    110\u001b[0m     eos \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/rerank/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/rerank/lib/python3.10/site-packages/exllamav2/model.py:582\u001b[0m, in \u001b[0;36mExLlamaV2.forward\u001b[0;34m(self, input_ids, cache, input_mask, preprocess_only, last_id_only, loras, return_last_state)\u001b[0m\n\u001b[1;32m    579\u001b[0m _last_id_only \u001b[39m=\u001b[39m last_id_only\n\u001b[1;32m    580\u001b[0m _preprocess_only \u001b[39m=\u001b[39m preprocess_only \u001b[39mor\u001b[39;00m (chunk_end \u001b[39m<\u001b[39m q_len \u001b[39mand\u001b[39;00m last_id_only)\n\u001b[0;32m--> 582\u001b[0m r, ls \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_forward(input_ids \u001b[39m=\u001b[39;49m input_ids[:, chunk_begin : chunk_end],\n\u001b[1;32m    583\u001b[0m                       cache \u001b[39m=\u001b[39;49m cache,\n\u001b[1;32m    584\u001b[0m                       input_mask \u001b[39m=\u001b[39;49m input_mask,\n\u001b[1;32m    585\u001b[0m                       preprocess_only \u001b[39m=\u001b[39;49m _preprocess_only,\n\u001b[1;32m    586\u001b[0m                       last_id_only \u001b[39m=\u001b[39;49m _last_id_only,\n\u001b[1;32m    587\u001b[0m                       loras \u001b[39m=\u001b[39;49m loras,\n\u001b[1;32m    588\u001b[0m                       return_last_state \u001b[39m=\u001b[39;49m return_last_state \u001b[39mand\u001b[39;49;00m remaining_q_len \u001b[39m<\u001b[39;49m\u001b[39m=\u001b[39;49m chunk_size)\n\u001b[1;32m    590\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m _preprocess_only:\n\u001b[1;32m    591\u001b[0m     result \u001b[39m=\u001b[39m r \u001b[39mif\u001b[39;00m result \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m torch\u001b[39m.\u001b[39mcat((result, r), dim \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/rerank/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/rerank/lib/python3.10/site-packages/exllamav2/model.py:655\u001b[0m, in \u001b[0;36mExLlamaV2._forward\u001b[0;34m(self, input_ids, cache, input_mask, preprocess_only, last_id_only, loras, return_last_state)\u001b[0m\n\u001b[1;32m    652\u001b[0m         last_state \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mnarrow(\u001b[39m-\u001b[39m\u001b[39m2\u001b[39m, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m)\n\u001b[1;32m    654\u001b[0m x \u001b[39m=\u001b[39m safe_move_tensor(x, device)\n\u001b[0;32m--> 655\u001b[0m x \u001b[39m=\u001b[39m module\u001b[39m.\u001b[39;49mforward(x, cache \u001b[39m=\u001b[39;49m cache, attn_mask \u001b[39m=\u001b[39;49m attn_mask, past_len \u001b[39m=\u001b[39;49m past_len, loras \u001b[39m=\u001b[39;49m loras)\n\u001b[1;32m    657\u001b[0m \u001b[39mif\u001b[39;00m preprocess_only \u001b[39mand\u001b[39;00m idx \u001b[39m==\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlast_kv_layer_idx:\n\u001b[1;32m    658\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/rerank/lib/python3.10/site-packages/exllamav2/attn.py:368\u001b[0m, in \u001b[0;36mExLlamaV2Attention.forward\u001b[0;34m(self, hidden_states, cache, attn_mask, past_len, intermediates, loras)\u001b[0m\n\u001b[1;32m    365\u001b[0m k_states \u001b[39m=\u001b[39m k_states\u001b[39m.\u001b[39mtranspose(\u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m)\n\u001b[1;32m    366\u001b[0m v_states \u001b[39m=\u001b[39m v_states\u001b[39m.\u001b[39mtranspose(\u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m)\n\u001b[0;32m--> 368\u001b[0m k_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrepeat_kv(k_states, num_key_value_groups)\n\u001b[1;32m    369\u001b[0m k_states \u001b[39m=\u001b[39m k_states\u001b[39m.\u001b[39mtranspose(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m-\u001b[39m\u001b[39m2\u001b[39m)\n\u001b[1;32m    371\u001b[0m attn_weights \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mmatmul(q_states, k_states)\n",
      "File \u001b[0;32m~/miniconda3/envs/rerank/lib/python3.10/site-packages/exllamav2/attn.py:236\u001b[0m, in \u001b[0;36mExLlamaV2Attention.repeat_kv\u001b[0;34m(self, hidden_states, n_rep)\u001b[0m\n\u001b[1;32m    233\u001b[0m \u001b[39mif\u001b[39;00m n_rep \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m: \u001b[39mreturn\u001b[39;00m hidden_states\n\u001b[1;32m    235\u001b[0m batch, num_key_value_heads, slen, head_dim \u001b[39m=\u001b[39m hidden_states\u001b[39m.\u001b[39mshape\n\u001b[0;32m--> 236\u001b[0m hidden_states \u001b[39m=\u001b[39m hidden_states[:, :, \u001b[39mNone\u001b[39;49;00m, :, :]\u001b[39m.\u001b[39;49mexpand(batch, num_key_value_heads, n_rep, slen, head_dim)\n\u001b[1;32m    237\u001b[0m hidden_states \u001b[39m=\u001b[39m hidden_states\u001b[39m.\u001b[39mreshape(batch, num_key_value_heads \u001b[39m*\u001b[39m n_rep, slen, head_dim)\n\u001b[1;32m    238\u001b[0m \u001b[39mreturn\u001b[39;00m hidden_states\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "reranker = Rerank(ExLlamaV2Reranker(), batch_size=128)\n",
    "rerank_results = reranker.rerank(corpus, queries, results, top_k=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save output\n",
    "output['reranking'] = EvaluateRetrieval.evaluate(qrels, rerank_results, retriever.k_values)\n",
    "\n",
    "with open(f'/content/{args.dataset}_eval_ouput.json', 'w') as f:\n",
    "    json.dump(output, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import logging\n",
    "\n",
    "top_k = 10\n",
    "\n",
    "\n",
    "query_id, ranking_scores = random.choice(list(rerank_results.items()))\n",
    "scores_sorted = sorted(ranking_scores.items(), key=lambda item: item[1], reverse=True)\n",
    "logging.info(\"Query : %s\\n\" % queries[query_id])\n",
    "\n",
    "for rank in range(top_k):\n",
    "  doc_id = scores_sorted[rank][0]\n",
    "  # Format: Rank x: ID [Title] Body\n",
    "  logging.info(\"Rank %d: %s [%s] - %s\\n\" % (rank+1, doc_id, corpus[doc_id].get(\"title\"), corpus[doc_id].get(\"text\")))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rerank",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
