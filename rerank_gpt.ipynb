{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import json\n",
    "import requests\n",
    "import tarfile\n",
    "import argparse\n",
    "import multiprocessing\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "\n",
    "import pyserini\n",
    "from pyserini.search import SimpleSearcher\n",
    "from pyserini.dsearch import SimpleDenseSearcher\n",
    "from pyserini.search.lucene import LuceneSearcher\n",
    "\n",
    "import transformers\n",
    "\n",
    "from peft import LoraConfig\n",
    "from transformers import (AutoTokenizer, \n",
    "                          AutoModelForCausalLM, \n",
    "                          BitsAndBytesConfig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(description='Rerank')\n",
    "\n",
    "parser.add_argument('--model_name', type=str, default='facebook/opt-125m')\n",
    "parser.add_argument('--collection', type=str, default='msmarco-passage')\n",
    "parser.add_argument('--collections_path', type=str, default='./collections/')\n",
    "parser.add_argument('--seed',type=int, default=42)\n",
    "parser.add_argument('--batch_size', type=int, default=256)\n",
    "parser.add_argument('--max_len', type=int, default=40)\n",
    "parser.add_argument('--lr', type=float, default=1e-5)\n",
    "parser.add_argument('--max_epochs', type=int, default=10)\n",
    "parser.add_argument('--use_cuda', type=bool, default=False)\n",
    "parser.add_argument('--k', type=int, default=100, help='top k')\n",
    "parser.add_argument('--k1', type=float, default=1.5, help='BM25 parameter')\n",
    "parser.add_argument('--b', type=float, default=0.75, help='BM25 parameter')\n",
    "\n",
    "parser.add_argument\n",
    "\n",
    "config = parser.parse_args([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_msmarco_passage_jsonl(collections_path, ):\n",
    "    msmarco_passage_path = os.path.join(collections_path, 'msmarco-passage')\n",
    "    # https://microsoft.github.io/msmarco/Datasets\n",
    "    msmarco_url = 'https://msmarco.blob.core.windows.net/msmarcoranking/collectionandqueries.tar.gz'\n",
    "    \n",
    "    if not os.path.exists(msmarco_passage_path):\n",
    "        os.mkdir(msmarco_passage_path)\n",
    "        \n",
    "    response = requests.get(msmarco_url, stream=True)\n",
    "    file = tarfile.open(fileobj=response.raw, mode='r|gz')\n",
    "    file.extractall(path=msmarco_passage_path)\n",
    "    \n",
    "    tsv_path = os.path.join(msmarco_passage_path, 'collection.tsv')\n",
    "    jsonl_path = os.path.join(msmarco_passage_path, 'collection_jsonl')\n",
    "    \n",
    "    if os.path.exists(tsv_path):\n",
    "        os.system(f'python anserini-tools/scripts/msmarco/convert_collection_to_jsonl.py ' +\n",
    "                  f'--collection-path {tsv_path} ' +\n",
    "                  f'--output-folder {jsonl_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_msmarco_passage_top1000():\n",
    "    top1000_tr_url = 'https://msmarco.blob.core.windows.net/msmarcoranking/top1000.train.tar.gz'\n",
    "    response = requests.get(top1000_tr_url, stream=True)\n",
    "    file = tarfile.open(fileobj=response.raw, mode='r|gz')\n",
    "    file.extractall(path='./collections/msmarco-passage')\n",
    "    \n",
    "    top1000_dev_url = 'https://msmarco.blob.core.windows.net/msmarcoranking/top1000.dev.tar.gz'\n",
    "    response = requests.get(top1000_dev_url, stream=True)\n",
    "    file = tarfile.open(fileobj=response.raw, mode='r|gz')\n",
    "    file.extractall(path='./collections/msmarco-passage')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'requests' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\cathy\\OneDrive\\바탕 화면\\rerank\\rerank_gpt.ipynb Cell 5\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/cathy/OneDrive/%EB%B0%94%ED%83%95%20%ED%99%94%EB%A9%B4/rerank/rerank_gpt.ipynb#X43sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m get_msmarco_passage_top1000()\n",
      "\u001b[1;32mc:\\Users\\cathy\\OneDrive\\바탕 화면\\rerank\\rerank_gpt.ipynb Cell 5\u001b[0m line \u001b[0;36m3\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/cathy/OneDrive/%EB%B0%94%ED%83%95%20%ED%99%94%EB%A9%B4/rerank/rerank_gpt.ipynb#X43sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_msmarco_passage_top1000\u001b[39m():\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/cathy/OneDrive/%EB%B0%94%ED%83%95%20%ED%99%94%EB%A9%B4/rerank/rerank_gpt.ipynb#X43sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     top1000_tr_url \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mhttps://msmarco.blob.core.windows.net/msmarcoranking/top1000.train.tar.gz\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/cathy/OneDrive/%EB%B0%94%ED%83%95%20%ED%99%94%EB%A9%B4/rerank/rerank_gpt.ipynb#X43sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     response \u001b[39m=\u001b[39m requests\u001b[39m.\u001b[39mget(top1000_tr_url, stream\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/cathy/OneDrive/%EB%B0%94%ED%83%95%20%ED%99%94%EB%A9%B4/rerank/rerank_gpt.ipynb#X43sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     file \u001b[39m=\u001b[39m tarfile\u001b[39m.\u001b[39mopen(fileobj\u001b[39m=\u001b[39mresponse\u001b[39m.\u001b[39mraw, mode\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mr|gz\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/cathy/OneDrive/%EB%B0%94%ED%83%95%20%ED%99%94%EB%A9%B4/rerank/rerank_gpt.ipynb#X43sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     file\u001b[39m.\u001b[39mextractall(path\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39m./collections/msmarco-passage\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'requests' is not defined"
     ]
    }
   ],
   "source": [
    "get_msmarco_passage_top1000()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "class Indexer:\n",
    "    def __init__(self, jsonl_path, index_path):\n",
    "        self.jsonl_path = jsonl_path\n",
    "        self.index_path = index_path # indexes/lucene-index-msmarco-passage\n",
    "    \n",
    "    def build_sparse_index(self):\n",
    "        execute_code = os.system('python -m pyserini.index.lucene ' + \n",
    "                                 '--collection JsonCollection ' +\n",
    "                                 f'--input {self.jsonl_path} ' +\n",
    "                                 f'--index {self.index_path} ' +\n",
    "                                 '--generator DefaultLuceneDocumentGenerator ' +\n",
    "                                 '--threads 1 --storeRaw')\n",
    "        if execute_code != 0:\n",
    "            raise Exception('Indexing Failed!')\n",
    "        else:\n",
    "            print('Indexing Success!')\n",
    "    \n",
    "    def build_dense_index(self):\n",
    "        pass \n",
    "\n",
    "class BM25Retriever:\n",
    "    def __init__(self, index_path, k, k1=1.5, b=0.75):\n",
    "        self.searcher = LuceneSearcher(index_path)\n",
    "        self.searcher.set_bm25(k1=k1, b=b)\n",
    "        self.k = k\n",
    "            \n",
    "    def _get_results(self, qid, hits:List):\n",
    "        results = []\n",
    "        \n",
    "        for i, hit in enumerate(hits):\n",
    "            docid = hit.docid\n",
    "            content = json.loads(hits[i].raw)['contents']\n",
    "            bm25_score = hit.score\n",
    "            result = {'rank': i+1,\n",
    "                      'qid': qid,\n",
    "                      'docid': docid, \n",
    "                      'score': bm25_score,\n",
    "                      'content': content}\n",
    "            results.append(result)\n",
    "            \n",
    "        return results\n",
    "    \n",
    "    def _save_json(self, results:List[dict]):\n",
    "        json_path = os.path.join('./retrieved/', f'bm25-{config.collection}-top{self.k}.json')\n",
    "        json_file = open(json_path, 'w', encoding='utf-8', newline='\\n')\n",
    "        for result in results:\n",
    "            json_file.write(json.dumps(result) + '\\n')\n",
    "        \n",
    "        json_file.close()\n",
    "    \n",
    "    def search(self, qid, query_text:str):\n",
    "        search_results = {}\n",
    "        hits = self.searcher.search(query_text, k=self.k,)\n",
    "        search_results['query'] = query_text\n",
    "        search_results['hits']  = self._get_results(qid, hits)\n",
    "        \n",
    "        return search_results\n",
    "    \n",
    "    def batch_search(self, qids:List[str], query_texts: List[str], is_save:bool):\n",
    "        query_dict = dict(zip(qids, query_texts))\n",
    "        batch_hits = self.searcher.batch_search(query_texts, qids, k=self.k, threads=multiprocessing.cpu_count())\n",
    "        bsearch_results = []\n",
    "        bsearch_items = {}\n",
    "\n",
    "        for qid, hits in batch_hits.items():\n",
    "            bsearch_items['query'] = query_dict[qid]\n",
    "            bsearch_items['hits'] = self._get_results(qid, hits)\n",
    "            bsearch_results.append(bsearch_items)\n",
    "            bsearch_items = {}\n",
    "            \n",
    "        if is_save:\n",
    "            self._save_json(bsearch_results)\n",
    "       \n",
    "        return bsearch_results\n",
    "    \n",
    "# if not os.path.exists(index_path):\n",
    "#             indexer = Indexer()\n",
    "#             self.build_sparse_index(jsonl_path, index_path)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_json(json_path:str):\n",
    "    with open(json_path, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        result = [json.loads(line) for line in lines]\n",
    "        return result\n",
    "\n",
    "def save_json(results:List[dict]):\n",
    "    json_path = os.path.join('./retrieved/', f'bm25-{config.collection}-top{config.k}.json')\n",
    "    json_file = open(json_path, 'w', encoding='utf-8', newline='\\n')\n",
    "    for result in results:\n",
    "        json_file.write(json.dumps(result) + '\\n')\n",
    "        \n",
    "    json_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection_dir_path = os.path.join(config.collections_path, config.collection)\n",
    "collection_path = os.path.join(collection_dir_path, 'collection.tsv')\n",
    "queries_train_path = os.path.join(collection_dir_path, 'queries.train.tsv')\n",
    "qrels_train_path = os.path.join(collection_dir_path, 'qrels.train.tsv')\n",
    "queries_dev_path = os.path.join(collection_dir_path, 'queries.dev.tsv')\n",
    "queries_eval_path = os.path.join(collection_dir_path, 'queries.eval.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries_train = pd.read_csv(queries_train_path, sep='\\t', header=None, names=['qid', 'query'])\n",
    "queries_train['qid'] = queries_train['qid'].astype(str)\n",
    "queries_dev = pd.read_csv(queries_dev_path, sep='\\t', header=None, names=['qid', 'query'])\n",
    "queries_dev['qid'] = queries_dev['qid'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "bm25 = BM25Retriever(index_path='indexes/lucene-index-msmarco-passage', k=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/21 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "# qids_tr = queries_train['qid'].tolist()\n",
    "# query_texts_tr = queries_train['query'].tolist()\n",
    "\n",
    "# results_tr = []\n",
    "\n",
    "# total = len(qids_tr)\n",
    "# batch_size = 40000\n",
    "# for i in tqdm(range(0, total, batch_size)):\n",
    "#     tmp = bm25.batch_search(qids=qids_tr[i:min(i + batch_size, total)], query_texts=query_texts_tr[i:min(i + batch_size, total)], is_save=False)\n",
    "#     results_tr.extend(tmp)\n",
    "    \n",
    "# save_json(results_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "808731"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# qids_dev = queries_dev['qid'].tolist()\n",
    "# query_texts_dev = queries_dev['query'].tolist()\n",
    "\n",
    "# results_dev = []\n",
    "\n",
    "# for i in tqdm(range(len(qids_dev))):\n",
    "#     tmp = bm25.search(qid=qids_dev[i], query_text=query_texts_dev[i])\n",
    "#     results_dev.append(tmp)\n",
    "    \n",
    "# save_json(results_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_result(results, qids, dids, scores):\n",
    "    for qid, did, score in zip(qids, dids, scores):\n",
    "        if qid not in results:\n",
    "            results[qid] = {}\n",
    "        \n",
    "        results[qid][did] = float(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MarcoDataset:\n",
    "    def __init__(self, collection_dir_path, tokenizer, mode='train'):\n",
    "        self.collection_dir_path = collection_dir_path\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.queries)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        query = self.queries.iloc[idx].query\n",
    "        corpus = self.collection.iloc[idx].corpus \n",
    "        \n",
    "        encoding = self.get_encoding(query, corpus, idx)\n",
    "    \n",
    "    def get_encoding(self, query, corpus, idx):\n",
    "        qids = self.tokenizer(query, max_length=128, truncation=True).input_ids\n",
    "        cids = self.tokenizer(corpus, max_length=512, truncation=True).input_ids\n",
    "        ids = cids + qids\n",
    "        encoding = self.tokenizer.encode()\n",
    "        return encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset, TensorDataset, IterableDataset\n",
    "\n",
    "class MarcoEncodedDataset(Dataset):\n",
    "    def __init__(self, collection_dir_path, tokenizer, mode='train', max_query_len=128, max_corpus_len=512):\n",
    "        self.collection_dir_path = collection_dir_path\n",
    "        self.tokenizer = tokenizer\n",
    "        self.mode = mode\n",
    "        self.max_query_len = max_query_len\n",
    "        self.max_corpus_len = max_corpus_len\n",
    "        # load data\n",
    "        collection_path = os.path.join(collection_dir_path, 'collection.tsv')\n",
    "        queries_path = os.path.join(collection_dir_path, f'queries.{mode}.tsv')\n",
    "        qrels_path = os.path.join(collection_dir_path, f'qrels.{mode}.tsv')\n",
    "        self.collection = pd.read_csv(collection_path, sep='\\t', header=None, names=['did', 'corpus'], index_col='did')\n",
    "        self.queries = pd.read_csv(queries_path, sep='\\t', header=None, names=['qid', 'query'], index_col='qid')\n",
    "        self.relations = pd.read_csv(qrels_path, sep='\\t', header=None, names=['qid', '0', 'did', 'label'])\n",
    "        \n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        x = self.queries.loc[x.qid].query_text\n",
    "        \n",
    "        query\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MarcoEncodedDataset(Dataset):\n",
    "    def __init__(self)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset, IterableDataset\n",
    "\n",
    "# https://github.com/OpenMatch/OpenMatch/blob/ad1d6228bcf288ebe86037f93cd4ae20061ec4ea/src/openmatch/retriever/reranker.py\n",
    "def RerankDataset(IterableDataset):\n",
    "    def __init__(self, tokenizer, query_dataset, corpus_dataset):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.query_dataset = query_dataset\n",
    "        self.corpus_dataset = corpus_dataset\n",
    "        \n",
    "    # def __iter__(self):\n",
    "    #     for qid, did in items():\n",
    "    #         yield \n",
    "    #         {\n",
    "    #                 \"query_id\": qid, \n",
    "    #                 \"doc_id\": did, \n",
    "    #                 **encode_pair(\n",
    "    #                     self.tokenizer, \n",
    "    #                     self.query_dataset[qid][\"input_ids\"], \n",
    "    #                     self.corpus_dataset[did][\"input_ids\"], \n",
    "    #                     self.query_dataset.max_len, \n",
    "    #                     self.corpus_dataset.max_len,\n",
    "    #                     encode_as_text_pair=self.encode_as_text_pair\n",
    "    #                 ),\n",
    "    #             }\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num = 100\n",
    "# 후에 query 후보를 여러 개 생성해서..? 더 많이.?\n",
    "# llama2 chat 활용해서\n",
    "# https://huggingface.co/TheBloke/Llama-2-13B-chat-GPTQ/discussions/5\n",
    "llama2_prompt = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are an intelligent assistant capable of generatig queries for given passages.\",\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": f\"Please generate a query for the following {num} passages based on its content. \\nThe task is to generate a query that summarizes the main points of each passage. \\nThe query should be relevant to the content of the passage.\"\n",
    "    },\n",
    "    {\"role\": \"assistant\", \"content\": \"Okay, please provide the passages to generate a query.\"},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.generate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTReranker:\n",
    "    def __init__(self):\n",
    "        self.model = self.load_model(config.model_name, config.use_cuda)\n",
    "        self.tokenizer = self.load_tokenizer(config.model_name)\n",
    "        self.model.eval()\n",
    "    \n",
    "    def load_model(self, model_name:str, use_cuda:bool):\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() & use_cuda else 'cpu')\n",
    "        model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float32).to(device)\n",
    "        model.config.use_cache=True\n",
    "        return model\n",
    "    \n",
    "    def load_tokenizer(self, model_name:str):\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        return tokenizer\n",
    "    \n",
    "    def _get_prompt(self, query)\n",
    "    \n",
    "    def rerank(self, query, texts):\n",
    "        prompt =  f\"Please generate a query based on the following passage: {texts}\"\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained('EleutherAI/gpt-neo-125m')\n",
    "tokenizer = AutoTokenizer.from_pretrained('EleutherAI/gpt-neo-125m')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "passages = \"Since 2007, the RBA's outstanding reputation has been affected by the 'Securency' or NPA scandal. These RBA subsidiaries were involved in bribing overseas officials so that Australia might win lucrative note-printing contracts. The assets of the bank include the gold and foreign exchange reserves of Australia, which is estimated to have a net worth of A$101 billion. Nearly 94% of the RBA's employees work at its headquarters in Sydney, New South Wales and at the Business Resumption Site.\"\n",
    "prompt =f\"Please generate a question for the following passages: {passages}\"\n",
    "\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(prompt, return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_ids = model.generate(inputs.input_ids, num_return_sequences=1, do_sample=True, num_beams=1, max_new_tokens=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode(generate_ids[0], skip_special_tokens=True,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "generate_ids = model.generate(inputs.input_ids, max_length=128)\n",
    "tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rerank(self, query, texts):\n",
    "        reranked_texts = []\n",
    "\n",
    "        # Encode the query text\n",
    "        query_inputs = self.tokenizer(query, return_tensors='pt', truncation=True, max_length=self.max_len, padding=True)\n",
    "\n",
    "        for text in texts:\n",
    "            # Encode the text\n",
    "            text_inputs = self.tokenizer(text, return_tensors='pt', truncation=True, max_length=self.max_len, padding=True)\n",
    "\n",
    "            # Generate the reranking input by concatenating query and text\n",
    "            rerank_input = {\n",
    "                'input_ids': torch.cat([query_inputs['input_ids'], text_inputs['input_ids']], dim=1),\n",
    "                'attention_mask': torch.cat([query_inputs['attention_mask'], text_inputs['attention_mask']], dim=1)\n",
    "            }\n",
    "\n",
    "            # Generate reranking scores using the GPT model\n",
    "            with torch.no_grad():\n",
    "                logits = self.model(**rerank_input).logits\n",
    "\n",
    "            # Calculate the total score by summing logits\n",
    "            total_score = logits.sum().item()\n",
    "\n",
    "            # Append text and total score to the reranked_texts\n",
    "            reranked_texts.append({'text': text, 'total_score': total_score})\n",
    "\n",
    "        # Sort texts based on total_score in descending order\n",
    "        reranked_texts.sort(key=lambda x: x['total_score'], reverse=True)\n",
    "\n",
    "        # Extract the sorted texts\n",
    "        sorted_texts = [item['text'] for item in reranked_texts]\n",
    "\n",
    "        return sorted_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rerank",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
